{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "    \"income_bracket\"\n",
    "]\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"gender\", [\"Female\", \"Male\"])\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"education\", [\n",
    "        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "        \"Preschool\", \"12th\"\n",
    "    ])\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"marital_status\", [\n",
    "        \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "        \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n",
    "    ])\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"relationship\", [\n",
    "        \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "        \"Other-relative\"\n",
    "    ])\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"workclass\", [\n",
    "        \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "        \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n",
    "    ])\n",
    "race = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"race\", [\n",
    "        \"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"\n",
    "    ])\n",
    "\n",
    "# To show an example of hashing:\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"occupation\", hash_bucket_size=1000)\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "# Continuous base columns.\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "fnlwgt = tf.feature_column.numeric_column(\"fnlwgt\")\n",
    "education_num = tf.feature_column.numeric_column(\"education_num\")\n",
    "capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n",
    "capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n",
    "hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")\n",
    "\n",
    "# Transformations.\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "# Wide columns and deep columns.\n",
    "base_columns = [\n",
    "    gender, education, marital_status, relationship, workclass, occupation,\n",
    "    native_country, age_buckets,\n",
    "]\n",
    "\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        [\"education\", \"occupation\"], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, \"education\", \"occupation\"], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [\"native_country\", \"occupation\"], hash_bucket_size=1000)\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    tf.feature_column.indicator_column(workclass),\n",
    "    tf.feature_column.indicator_column(marital_status),\n",
    "    tf.feature_column.indicator_column(education),\n",
    "    tf.feature_column.indicator_column(gender),\n",
    "    tf.feature_column.indicator_column(relationship),\n",
    "    tf.feature_column.indicator_column(race),\n",
    "    # To show an example of embedding\n",
    "    tf.feature_column.embedding_column(native_country, dimension=8),\n",
    "    tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "#     fnlwgt,\n",
    "    age,\n",
    "    education_num,\n",
    "    capital_gain,\n",
    "    capital_loss,\n",
    "    hours_per_week,\n",
    "]\n",
    "\n",
    "def maybe_download(train_data, test_data):\n",
    "  \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "  if train_data:\n",
    "    train_file_name = train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "        train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if test_data:\n",
    "    test_file_name = test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "        test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\"% test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  if model_type == \"wide\":\n",
    "    m = tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir, feature_columns=base_columns + crossed_columns)\n",
    "  elif model_type == \"deep\":\n",
    "    m = tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "#         optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "        hidden_units=[100, 75, 50, 25],\n",
    "#         dropout = 0.4,\n",
    "        config=tf.estimator.RunConfig(tf_random_seed=1234))\n",
    "  else:\n",
    "    m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=crossed_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])\n",
    "  return m\n",
    "\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  df_data = pd.read_csv(\n",
    "      tf.gfile.Open(data_file),\n",
    "      names=CSV_COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\",\n",
    "      skiprows=1,\n",
    "#       sep = ' ',\n",
    "#       header=0\n",
    "  )\n",
    "  # remove NaN elements\n",
    "  df_data = df_data.dropna(how=\"any\", axis=0)\n",
    "  labels = df_data[\"income_bracket\"].apply(lambda x: \">50K\" in x).astype(int)\n",
    "  return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df_data,\n",
    "      y=labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=num_epochs,\n",
    "      shuffle=shuffle,\n",
    "      num_threads=5)\n",
    "\n",
    "\n",
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "  model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "\n",
    "  m = build_estimator(model_dir, model_type)\n",
    "  # set num_epochs to None to get infinite stream of data.\n",
    "  m.train(\n",
    "      input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True),\n",
    "      steps=train_steps)\n",
    "  # set steps to None to run evaluation until all data consumed.\n",
    "  results = m.evaluate(\n",
    "      input_fn=input_fn(test_file_name, num_epochs=1, shuffle=False),\n",
    "      steps=None)\n",
    "  results_train = m.evaluate(\n",
    "      input_fn=input_fn(train_file_name, num_epochs=1, shuffle=False),\n",
    "      steps=None)\n",
    "#   print(\"model directory = %s\" % model_dir)\n",
    "#   for key in sorted(results):\n",
    "#     print(\"%s: %s\" % (key, results[key]))\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "# def main(_):\n",
    "#   train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "#                  FLAGS.train_data, FLAGS.test_data)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   parser = argparse.ArgumentParser()\n",
    "#   parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "#   parser.add_argument(\n",
    "#       \"--model_dir\",\n",
    "#       type=str,\n",
    "#       default=\"\",\n",
    "#       help=\"Base directory for output models.\"\n",
    "#   )\n",
    "#   parser.add_argument(\n",
    "#       \"--model_type\",\n",
    "#       type=str,\n",
    "#       default=\"wide_n_deep\",\n",
    "#       help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    "#   )\n",
    "#   parser.add_argument(\n",
    "#       \"--train_steps\",\n",
    "#       type=int,\n",
    "#       default=2000,\n",
    "#       help=\"Number of training steps.\"\n",
    "#   )\n",
    "#   parser.add_argument(\n",
    "#       \"--train_data\",\n",
    "#       type=str,\n",
    "#       default=\"\",\n",
    "#       help=\"Path to the training data.\"\n",
    "#   )\n",
    "#   parser.add_argument(\n",
    "#       \"--test_data\",\n",
    "#       type=str,\n",
    "#       default=\"\",\n",
    "#       help=\"Path to the test data.\"\n",
    "#   )\n",
    "#   FLAGS, unparsed = parser.parse_known_args()\n",
    "#   tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'D:\\Study\\Ostfold\\MachineLearning\\git\\data\\\\splitted_data.train'\n",
    "VAL_FILE = 'D:\\Study\\Ostfold\\MachineLearning\\git\\data\\\\splitted_data.val'\n",
    "TEST_FILE = 'D:\\Study\\Ostfold\\MachineLearning\\git\\data\\\\splitted_data.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is downloaded to C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp3o52_0t6\n",
      "Test data is downloaded to C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpixtersyb\n",
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Local\\\\Temp\\\\tmpiede82s_', '_task_type': 'worker', '_log_step_count_steps': 100, '_service': None, '_save_checkpoints_steps': None, '_session_config': None, '_tf_random_seed': 1234, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000274D0413E10>, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpiede82s_\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 876.6583\n",
      "INFO:tensorflow:global_step/sec: 164.737\n",
      "INFO:tensorflow:step = 101, loss = 40.264206 (0.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.346\n",
      "INFO:tensorflow:step = 201, loss = 109.86388 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.132\n",
      "INFO:tensorflow:step = 301, loss = 42.674393 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.342\n",
      "INFO:tensorflow:step = 401, loss = 34.913765 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.778\n",
      "INFO:tensorflow:step = 501, loss = 32.4104 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.254\n",
      "INFO:tensorflow:step = 601, loss = 38.63565 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.662\n",
      "INFO:tensorflow:step = 701, loss = 47.44254 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.517\n",
      "INFO:tensorflow:step = 801, loss = 29.318386 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.626\n",
      "INFO:tensorflow:step = 901, loss = 47.443474 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.915\n",
      "INFO:tensorflow:step = 1001, loss = 32.638107 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.178\n",
      "INFO:tensorflow:step = 1101, loss = 30.94421 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.297\n",
      "INFO:tensorflow:step = 1201, loss = 28.96771 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.01\n",
      "INFO:tensorflow:step = 1301, loss = 32.526237 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.854\n",
      "INFO:tensorflow:step = 1401, loss = 24.096996 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.968\n",
      "INFO:tensorflow:step = 1501, loss = 26.407677 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.962\n",
      "INFO:tensorflow:step = 1601, loss = 36.7481 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.425\n",
      "INFO:tensorflow:step = 1701, loss = 40.063705 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.279\n",
      "INFO:tensorflow:step = 1801, loss = 29.227617 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.816\n",
      "INFO:tensorflow:step = 1901, loss = 28.909506 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.15\n",
      "INFO:tensorflow:step = 2001, loss = 30.864702 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.542\n",
      "INFO:tensorflow:step = 2101, loss = 32.54859 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.634\n",
      "INFO:tensorflow:step = 2201, loss = 29.81808 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.088\n",
      "INFO:tensorflow:step = 2301, loss = 46.028717 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.318\n",
      "INFO:tensorflow:step = 2401, loss = 28.317822 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.687\n",
      "INFO:tensorflow:step = 2501, loss = 41.423737 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.41\n",
      "INFO:tensorflow:step = 2601, loss = 31.601387 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.412\n",
      "INFO:tensorflow:step = 2701, loss = 44.602142 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.72\n",
      "INFO:tensorflow:step = 2801, loss = 41.40328 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.156\n",
      "INFO:tensorflow:step = 2901, loss = 26.817566 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.502\n",
      "INFO:tensorflow:step = 3001, loss = 28.641857 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.483\n",
      "INFO:tensorflow:step = 3101, loss = 28.388756 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.848\n",
      "INFO:tensorflow:step = 3201, loss = 33.6086 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.808\n",
      "INFO:tensorflow:step = 3301, loss = 30.15802 (0.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.203\n",
      "INFO:tensorflow:step = 3401, loss = 29.707172 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.489\n",
      "INFO:tensorflow:step = 3501, loss = 29.039139 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.959\n",
      "INFO:tensorflow:step = 3601, loss = 37.565983 (0.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.521\n",
      "INFO:tensorflow:step = 3701, loss = 22.729103 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.627\n",
      "INFO:tensorflow:step = 3801, loss = 26.214571 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.403\n",
      "INFO:tensorflow:step = 3901, loss = 24.493649 (0.665 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.574\n",
      "INFO:tensorflow:step = 4001, loss = 34.92713 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.256\n",
      "INFO:tensorflow:step = 4101, loss = 32.277508 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.955\n",
      "INFO:tensorflow:step = 4201, loss = 24.41169 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.193\n",
      "INFO:tensorflow:step = 4301, loss = 27.870457 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.866\n",
      "INFO:tensorflow:step = 4401, loss = 31.059694 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.13\n",
      "INFO:tensorflow:step = 4501, loss = 32.6575 (0.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.688\n",
      "INFO:tensorflow:step = 4601, loss = 26.553787 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.133\n",
      "INFO:tensorflow:step = 4701, loss = 39.10755 (0.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.492\n",
      "INFO:tensorflow:step = 4801, loss = 27.076138 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.177\n",
      "INFO:tensorflow:step = 4901, loss = 32.50011 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.664\n",
      "INFO:tensorflow:step = 5001, loss = 41.412727 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.559\n",
      "INFO:tensorflow:step = 5101, loss = 32.33039 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.549\n",
      "INFO:tensorflow:step = 5201, loss = 36.27656 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.765\n",
      "INFO:tensorflow:step = 5301, loss = 34.555832 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.306\n",
      "INFO:tensorflow:step = 5401, loss = 22.348488 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.063\n",
      "INFO:tensorflow:step = 5501, loss = 50.147385 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.177\n",
      "INFO:tensorflow:step = 5601, loss = 37.596554 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.09\n",
      "INFO:tensorflow:step = 5701, loss = 35.47181 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.645\n",
      "INFO:tensorflow:step = 5801, loss = 32.205467 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.281\n",
      "INFO:tensorflow:step = 5901, loss = 26.206099 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.638\n",
      "INFO:tensorflow:step = 6001, loss = 36.471626 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.677\n",
      "INFO:tensorflow:step = 6101, loss = 27.189373 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.671\n",
      "INFO:tensorflow:step = 6201, loss = 29.354055 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.806\n",
      "INFO:tensorflow:step = 6301, loss = 20.30626 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.813\n",
      "INFO:tensorflow:step = 6401, loss = 28.153406 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.866\n",
      "INFO:tensorflow:step = 6501, loss = 29.293215 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.415\n",
      "INFO:tensorflow:step = 6601, loss = 30.785805 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.411\n",
      "INFO:tensorflow:step = 6701, loss = 28.111454 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.623\n",
      "INFO:tensorflow:step = 6801, loss = 27.717594 (0.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.183\n",
      "INFO:tensorflow:step = 6901, loss = 31.88401 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.72\n",
      "INFO:tensorflow:step = 7001, loss = 20.253222 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.682\n",
      "INFO:tensorflow:step = 7101, loss = 37.98666 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.41\n",
      "INFO:tensorflow:step = 7201, loss = 32.82314 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.278\n",
      "INFO:tensorflow:step = 7301, loss = 23.141312 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.514\n",
      "INFO:tensorflow:step = 7401, loss = 50.299103 (0.519 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 198.923\n",
      "INFO:tensorflow:step = 7501, loss = 35.709274 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.871\n",
      "INFO:tensorflow:step = 7601, loss = 28.936262 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.158\n",
      "INFO:tensorflow:step = 7701, loss = 31.582924 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.027\n",
      "INFO:tensorflow:step = 7801, loss = 31.568047 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.318\n",
      "INFO:tensorflow:step = 7901, loss = 24.28786 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.148\n",
      "INFO:tensorflow:step = 8001, loss = 34.43012 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.641\n",
      "INFO:tensorflow:step = 8101, loss = 30.641726 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.36\n",
      "INFO:tensorflow:step = 8201, loss = 37.40615 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.168\n",
      "INFO:tensorflow:step = 8301, loss = 35.108215 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.009\n",
      "INFO:tensorflow:step = 8401, loss = 27.450172 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.313\n",
      "INFO:tensorflow:step = 8501, loss = 62.336433 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.141\n",
      "INFO:tensorflow:step = 8601, loss = 28.437832 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.51\n",
      "INFO:tensorflow:step = 8701, loss = 31.69283 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.308\n",
      "INFO:tensorflow:step = 8801, loss = 28.207584 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.22\n",
      "INFO:tensorflow:step = 8901, loss = 33.694675 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.429\n",
      "INFO:tensorflow:step = 9001, loss = 24.430996 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.415\n",
      "INFO:tensorflow:step = 9101, loss = 23.472887 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.314\n",
      "INFO:tensorflow:step = 9201, loss = 32.090584 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.766\n",
      "INFO:tensorflow:step = 9301, loss = 23.68893 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.602\n",
      "INFO:tensorflow:step = 9401, loss = 35.200035 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.005\n",
      "INFO:tensorflow:step = 9501, loss = 24.57683 (0.501 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.295\n",
      "INFO:tensorflow:step = 9601, loss = 27.216373 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.29\n",
      "INFO:tensorflow:step = 9701, loss = 35.400055 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.318\n",
      "INFO:tensorflow:step = 9801, loss = 35.055286 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.833\n",
      "INFO:tensorflow:step = 9901, loss = 29.183086 (0.474 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpiede82s_\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 24.903889.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-09-13:35:35\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpiede82s_\\model.ckpt-10000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-09-13:35:40\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.8505006, accuracy_baseline = 0.76377374, auc = 0.9044119, auc_precision_recall = 0.76130277, average_loss = 0.31983036, global_step = 10000, label/mean = 0.23622628, loss = 31.945755, prediction/mean = 0.23768719\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-09-13:35:45\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmpiede82s_\\model.ckpt-10000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-09-13:35:53\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.85816956, accuracy_baseline = 0.75918305, auc = 0.91605145, auc_precision_recall = 0.7849083, average_loss = 0.30779973, global_step = 10000, label/mean = 0.24081695, loss = 30.779974, prediction/mean = 0.24065602\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(\"\", \"deep\", 10000, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_num_ps_replicas': 0, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B979C74AC8>, '_tf_random_seed': None, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_model_dir': 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Local\\\\Temp\\\\tmp_o07q5nm', '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_service': None, '_is_chief': True, '_task_type': 'worker'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp_o07q5nm\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 3060.7476\n",
      "INFO:tensorflow:global_step/sec: 139.723\n",
      "INFO:tensorflow:step = 101, loss = 53.771553 (0.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.522\n",
      "INFO:tensorflow:step = 201, loss = 50.86369 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.643\n",
      "INFO:tensorflow:step = 301, loss = 43.7292 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.045\n",
      "INFO:tensorflow:step = 401, loss = 47.664024 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.157\n",
      "INFO:tensorflow:step = 501, loss = 64.826645 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.682\n",
      "INFO:tensorflow:step = 601, loss = 78.893776 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.86\n",
      "INFO:tensorflow:step = 701, loss = 47.48545 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.864\n",
      "INFO:tensorflow:step = 801, loss = 36.633026 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.879\n",
      "INFO:tensorflow:step = 901, loss = 37.284126 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.149\n",
      "INFO:tensorflow:step = 1001, loss = 40.254265 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.897\n",
      "INFO:tensorflow:step = 1101, loss = 31.918932 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.679\n",
      "INFO:tensorflow:step = 1201, loss = 44.456802 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.243\n",
      "INFO:tensorflow:step = 1301, loss = 62.426544 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.881\n",
      "INFO:tensorflow:step = 1401, loss = 46.376015 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.102\n",
      "INFO:tensorflow:step = 1501, loss = 41.184666 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.133\n",
      "INFO:tensorflow:step = 1601, loss = 36.16774 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.237\n",
      "INFO:tensorflow:step = 1701, loss = 42.158516 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.835\n",
      "INFO:tensorflow:step = 1801, loss = 33.9673 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.336\n",
      "INFO:tensorflow:step = 1901, loss = 36.687016 (0.538 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp_o07q5nm\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 42.166897.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-01-02:08:04\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp_o07q5nm\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-01-02:08:07\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.8382985, accuracy_baseline = 0.75429976, auc = 0.8793438, auc_precision_recall = 0.73296446, average_loss = 0.36545146, global_step = 2000, label/mean = 0.24570024, loss = 36.500305, prediction/mean = 0.25874954\n",
      "model directory = C:\\Users\\ADMIN\\AppData\\Local\\Temp\\tmp_o07q5nm\n",
      "accuracy: 0.8382985\n",
      "accuracy_baseline: 0.75429976\n",
      "auc: 0.8793438\n",
      "auc_precision_recall: 0.73296446\n",
      "average_loss: 0.36545146\n",
      "global_step: 2000\n",
      "label/mean: 0.24570024\n",
      "loss: 36.500305\n",
      "prediction/mean: 0.25874954\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(\"\", \"wide_n_deep\", 2000, TRAIN_FILE, TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
